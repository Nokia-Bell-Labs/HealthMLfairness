{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Â© 2024 Nokia\n",
    "Licensed under the BSD 3 Clause Clear License  \n",
    "SPDX-License-Identifier: BSD-3-Clause-Clear"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 1,
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from definitions import ROOT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "sns.set(font_scale = 1.5)\n",
    "\n",
    "seed = 2\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MODEL"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"base_model_simclrlinear\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 101, 5)]          0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 78, 32)            3872      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 78, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 63, 64)            32832     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 63, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 56, 96)            49248     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 56, 96)            0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               12416     \n",
      "_________________________________________________________________\n",
      "drop_0 (Dropout)             (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "softmax (Softmax)            (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 98,626\n",
      "Trainable params: 12,674\n",
      "Non-trainable params: 85,952\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Dataset-specific\n",
    "data_folder = 'MESA'\n",
    "working_directory = os.path.join('SimCLR', data_folder)\n",
    "# SimCLR finetuned model\n",
    "subfolder = '20230330-110822_l2_hs128_e100_esFalse_bs128_wTrue_rFalse'\n",
    "model_name = 'simclr.finetuned.0.80.hdf5'\n",
    "frozen_layers = ''\n",
    "added_layers = 2\n",
    "tag = 'finetuned2'\n",
    "pretrained_model = tf.keras.models.load_model(os.path.join(working_directory, subfolder, model_name), compile=False)  # compile=False as we use the model only for inference\n",
    "image_folder = os.path.join(data_folder, 'img', subfolder)\n",
    "if not os.path.exists(image_folder):\n",
    "    os.makedirs(image_folder)\n",
    "pretrained_model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DATA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "np_train = (np.load(os.path.join(working_directory, 'train_x.npy')),\n",
    "           np.load(os.path.join(working_directory, 'train_y.npy')))\n",
    "np_val = (np.load(os.path.join(working_directory, 'val_x.npy')),\n",
    "           np.load(os.path.join(working_directory, 'val_y.npy')))\n",
    "np_test = (np.load(os.path.join(working_directory, 'test_x.npy')),\n",
    "           np.load(os.path.join(working_directory, 'test_y.npy')))\n",
    "\n",
    "probs = pretrained_model.predict(np_test[0])\n",
    "predictions = np.argmax(probs, axis=1)\n",
    "# print(simclr_utitlities.evaluate_model_simple(pretrained_model.predict(np_test[0]), np_test[1], return_dict=True))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Validation - Test Set Shapes:\n",
      "Train X: (1449147, 101, 5) - Val X: (365497, 101, 5) - Test X: (452015, 101, 5)\n",
      "Train y: (1449147, 2) - Val y: (365497, 2) - Test y: (452015, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train - Validation - Test Set Shapes:\")\n",
    "print(\"Train X: {} - Val X: {} - Test X: {}\".format(np_train[0].shape, np_val[0].shape, np_test[0].shape))\n",
    "print(\"Train y: {} - Val y: {} - Test y: {}\".format(np_train[1].shape, np_val[1].shape, np_test[1].shape))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'datasets\\\\MESA\\\\train_listfile.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m subjects \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../../datasets\u001B[39m\u001B[38;5;124m'\u001B[39m, data_folder, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdemographics.csv\u001B[39m\u001B[38;5;124m'\u001B[39m), delimiter\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m;\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# subject IDs per train-validation-test set\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m train_listfile \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdatasets\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_folder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtrain_listfile.csv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m val_listfile \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdatasets\u001B[39m\u001B[38;5;124m'\u001B[39m, data_folder, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_listfile.csv\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[0;32m      6\u001B[0m test_listfile \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdatasets\u001B[39m\u001B[38;5;124m'\u001B[39m, data_folder, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest_listfile.csv\u001B[39m\u001B[38;5;124m'\u001B[39m))\n",
      "File \u001B[1;32m~\\PycharmProjects\\TFC-pretraining\\venv\\lib\\site-packages\\pandas\\io\\parsers.py:688\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001B[0m\n\u001B[0;32m    635\u001B[0m     engine_specified \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    637\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(\n\u001B[0;32m    638\u001B[0m     delimiter\u001B[38;5;241m=\u001B[39mdelimiter,\n\u001B[0;32m    639\u001B[0m     engine\u001B[38;5;241m=\u001B[39mengine,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    685\u001B[0m     skip_blank_lines\u001B[38;5;241m=\u001B[39mskip_blank_lines,\n\u001B[0;32m    686\u001B[0m )\n\u001B[1;32m--> 688\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\TFC-pretraining\\venv\\lib\\site-packages\\pandas\\io\\parsers.py:454\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    451\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    453\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[1;32m--> 454\u001B[0m parser \u001B[38;5;241m=\u001B[39m \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfp_or_buf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    456\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[0;32m    457\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[1;32m~\\PycharmProjects\\TFC-pretraining\\venv\\lib\\site-packages\\pandas\\io\\parsers.py:948\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m    945\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m kwds:\n\u001B[0;32m    946\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m--> 948\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\TFC-pretraining\\venv\\lib\\site-packages\\pandas\\io\\parsers.py:1180\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[1;34m(self, engine)\u001B[0m\n\u001B[0;32m   1178\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_make_engine\u001B[39m(\u001B[38;5;28mself\u001B[39m, engine\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mc\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m   1179\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m engine \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mc\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m-> 1180\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[43mCParserWrapper\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1181\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1182\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m engine \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpython\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\TFC-pretraining\\venv\\lib\\site-packages\\pandas\\io\\parsers.py:2010\u001B[0m, in \u001B[0;36mCParserWrapper.__init__\u001B[1;34m(self, src, **kwds)\u001B[0m\n\u001B[0;32m   2007\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39musecols, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39musecols_dtype \u001B[38;5;241m=\u001B[39m _validate_usecols_arg(kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124musecols\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m   2008\u001B[0m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124musecols\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39musecols\n\u001B[1;32m-> 2010\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reader \u001B[38;5;241m=\u001B[39m \u001B[43mparsers\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTextReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2011\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39munnamed_cols \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reader\u001B[38;5;241m.\u001B[39munnamed_cols\n\u001B[0;32m   2013\u001B[0m passed_names \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnames \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mpandas\\_libs\\parsers.pyx:382\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader.__cinit__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\parsers.pyx:674\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'datasets\\\\MESA\\\\train_listfile.csv'"
     ]
    }
   ],
   "source": [
    "# os.chdir(ROOT_DIR)\n",
    "subjects = pd.read_csv(os.path.join('../../datasets', data_folder, 'demographics.csv'), delimiter=';')\n",
    "# subject IDs per train-validation-test set\n",
    "train_listfile = pd.read_csv(os.path.join('datasets', data_folder, 'train_listfile.csv'))\n",
    "val_listfile = pd.read_csv(os.path.join('datasets', data_folder, 'val_listfile.csv'))\n",
    "test_listfile = pd.read_csv(os.path.join('datasets', data_folder, 'test_listfile.csv'))\n",
    "# change back the working directory\n",
    "os.chdir(os.path.join('code', 'baselines', 'SimCLR'))\n",
    "subjects.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_listfile"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# adding predictions\n",
    "test_listfile.loc[:, \"y_pred\"] = predictions\n",
    "test_listfile.loc[:, \"y_prob\"] = probs\n",
    "test_listfile.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# HOW MANY SAMPLES PER USER"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_listfile.mesaid.value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Unique users: {} vs. Total samples: {}\".format(len(test_listfile.mesaid.unique()), test_listfile.shape[0]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Max samples per user: {} vs. Min samples per user: {}\".format(test_listfile.mesaid.value_counts().max(), test_listfile.mesaid.value_counts().min()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# AGGREGATION\n",
    "Options:\n",
    "Get the mode (y_true, y_pred) per user\n",
    "Get the weighted fairness metric based on the #samples per user"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MODE (w/ argmax)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_listfile_aggregated = test_listfile.groupby('mesaid')[['wake', 'y_pred']].agg(pd.Series.mode)\n",
    "test_listfile_aggregated.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# indeed the rows are as many as the users\n",
    "test_listfile_aggregated.shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MEDIAN (w/ probabilities)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_listfile_aggregated_median = test_listfile.groupby('mesaid')[['y_prob']].agg(pd.Series.median)\n",
    "test_listfile_aggregated.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# merge demographics in test df\n",
    "test_listfile_aggregated_median = test_listfile_aggregated_median.merge(subjects, on=\"mesaid\", how=\"left\")\n",
    "test_listfile_aggregated_median.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# adding new age attribute\n",
    "test_listfile_aggregated_median.loc[:, 'nsrr_age_gt65'] = test_listfile_aggregated_median.nsrr_age.map(lambda age: 'no' if age < 65 else 'yes')\n",
    "test_listfile_aggregated_median.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# HOW DIFFERENT ARE THE PREDICTED DISTRIBUTIONS NOW?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.hist(test_listfile_aggregated_median.loc[test_listfile_aggregated_median.nsrr_sex == \"female\", \"y_prob\"], alpha=0.5, label='Aggregated')\n",
    "plt.hist(test_listfile_aggregated_median.loc[test_listfile_aggregated_median.nsrr_sex == \"female\", \"y_prob\"], alpha=0.5, label='Raw')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
